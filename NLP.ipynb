{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "Natural language processing is a field concerned with the ability of a computer to understand, analyze, manipulate, and potentially generate human language. By human language, we're simply referring to any language used for everyday communication. \n",
    "\n",
    "\n",
    "So here are a few examples that you may see on a day to day basis:\n",
    "\n",
    ">The first would be a spam filter, so this is just where your email server is determining whether an incoming email is spam or not, based on the content of the body, the subject, and maybe the email domain. \n",
    "\n",
    ">The second is auto-complete, where Google is basically predicting what you're interested in searching for based on what you've already entered and what others commonly search for with those same phrases. \n",
    "\n",
    "\n",
    "NLP is a very broad umbrella that encompasses many topics like the following:\n",
    "\n",
    "* Sentiment Analysis\n",
    "* Topic Modeling\n",
    "* Text Classification\n",
    "* Sentence Segmentation or Part-or-Speech Tagging\n",
    "\n",
    "The core component of natural language processing is extracting all the information from a block of text that is relevant to a computer understanding the language.\n",
    "\n",
    "I used the following command to install NLTK:\n",
    "\n",
    "- !pip3 install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "pd.set_option('display.max_colwidth', 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it only once, and download everything by selecting them and then clicking on the download.\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see many functions in the NLTK library, one of which is stopwords.\n",
    "\n",
    "\n",
    "> Stop words are basically words that are used very frequently but don't really contribute much to the meaning of a sentence. \n",
    "\n",
    "> For instance, you may be trying to do some sentiment analysis, and these words are generally sentiment-neutral. So there's no strong meaning necessarily one way or the other. They're just clouding the signal and taking room away from words that aren't sentiment-neutral. So we can go ahead and safely drop these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'herself', 'been', 'with', 'here', 'very', 'doesn', 'won']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# start at the zeroth element, or the element in the zeroth position, and go up through the element \n",
    "# in the 500th position, and we only want it to print out in increments of 25\n",
    "\n",
    "print(stopwords.words(\"english\")[0:500:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tNah I don't think he goes to usf, he lives around here though\\nham\\tEven my brother is not like to speak with me. They treat me like aid\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData = open(\"SMSSpamCollection.tsv\").read()\n",
    "\n",
    "# View the data\n",
    "\n",
    "rawData[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above output, we can see,\n",
    "\n",
    "> You could see that it's just basically a block of text, you have these \\t and these \\n separators. \n",
    "\n",
    "> The \\t's are between the labels and the text message bodies, and the \\n's are typically at the end of those lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with these \\n and \\t, we are going to replace the \\t's with \\n's, and that'll allow us to split this into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n",
       " 'spam',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'ham']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData = rawData.replace(\"\\t\", \"\\n\").split(\"\\n\")\n",
    "parsedData[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above output, we know that, \n",
    "> Every item at the even index (starting from zero) is a label list, either ham or spam. \n",
    "\n",
    "> While, every odd index, is a text\n",
    "\n",
    "We need to seperate them, and we can do that by slicing the parsedData as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'spam', 'ham', 'ham', 'ham']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelList = parsedData[0::2]\n",
    "labelList[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textList = parsedData[1::2]\n",
    "textList[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to move ahead, we need to have same length of labelList and textList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of labelList:  5571\n",
      "Length of textList :  5570\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of labelList: \",len(labelList))\n",
    "print(\"Length of textList : \",len(textList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with the unequal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham', 'ham', 'ham', 'ham', '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelList[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We're going to create a data frame, and within this data frame, we're going to need to pass it a dictionary, where, in the dictionary, the keys are going to be the name of the columns, and then the values will be the lists that we've stored the actual values.\n",
    "\n",
    "> Note that, we need to have same length of labelList, and textList in order to create a proper dataFrame. In order to do so, we will ignore the last value of labelList, while creating the dataFrame. This can be achieved by passing labelList [: -1] (Everything except the last entry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                    body_text  \n",
       "0  I've been searching for the right words to thank you for this breather....  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...  \n",
       "2               Nah I don't think he goes to usf, he lives around here though  \n",
       "3  Even my brother is not like to speak with me. They treat me like aids p...  \n",
       "4                                         I HAVE A DATE ON SUNDAY WITH WILL!!  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullCorpus = pd.DataFrame({\n",
    "    \"label\": labelList[:-1],\n",
    "    \"body_text\": textList\n",
    "})\n",
    "\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an easy way to do the dataframe conversion for the given dataset. We can see from the output of rawData, there was an \"\\t\", which suggested that that the given dataset has a tab seperated delimiter.\n",
    "\n",
    "We can simply use pandas read_csv function, with the parameters seperator = \"\\t\", and header = None (Since the given dataset has no column names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                    body_text  \n",
       "0  I've been searching for the right words to thank you for this breather....  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...  \n",
       "2               Nah I don't think he goes to usf, he lives around here though  \n",
       "3  Even my brother is not like to speak with me. They treat me like aids p...  \n",
       "4                                         I HAVE A DATE ON SUNDAY WITH WILL!!  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"SMSSpamCollection.tsv\", sep = \"\\t\", header = None)\n",
    "dataset.columns = [\"label\", \"body_text\"]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy understanding, we will work on fullCorpus dataframe from here on out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the shape of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fullCorpus dataset has 5570 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "rows, column = fullCorpus.shape\n",
    "\n",
    "print(\"fullCorpus dataset has {} rows and {} columns\".format(rows, column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How many spam and ham values are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the 5570 rows in the dataset, 4824 are Ham, and 746 are Spam\n"
     ]
    }
   ],
   "source": [
    "print(\"Out of the {} rows in the dataset, {} are Ham, and {} are Spam\".format(len(fullCorpus),\n",
    "                                                                    len(fullCorpus[fullCorpus[\"label\"]==\"ham\"]),\n",
    "                                                                    len(fullCorpus[fullCorpus[\"label\"]==\"spam\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How many missing values are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null values in Label    : 0\n",
      "Number of Null values in body_text: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Null values in Label    : {}\".format(fullCorpus[\"label\"].isnull().sum()))\n",
    "print(\"Number of Null values in body_text: {}\".format(fullCorpus[\"body_text\"].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing text data\n",
    "\n",
    "Cleaning up the text data is necessary to highlight attributes that you're going to want your machine learning system to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:\n",
    "1. **Remove punctuation**\n",
    "2. **Tokenization**\n",
    "3. **Remove stopwords**\n",
    "4. Lemmatize/Stem\n",
    "\n",
    "Note that, Lemmatizing and stemming are helpful but not critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \n",
    "    nopunct_text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    return nopunct_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather....</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids p...</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                    body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather....   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...   \n",
       "2               Nah I don't think he goes to usf, he lives around here though   \n",
       "3  Even my brother is not like to speak with me. They treat me like aids p...   \n",
       "4                                         I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                            body_text_nopunct  \n",
       "0  Ive been searching for the right words to thank you for this breather I...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text...  \n",
       "2                 Nah I dont think he goes to usf he lives around here though  \n",
       "3  Even my brother is not like to speak with me They treat me like aids pa...  \n",
       "4                                           I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"body_text_nopunct\"] = dataset[\"body_text\"].apply(lambda x: remove_punctuation(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    tokens = re.split('\\W+', text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunct</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather....</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids p...</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids pa...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                    body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather....   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...   \n",
       "2               Nah I don't think he goes to usf, he lives around here though   \n",
       "3  Even my brother is not like to speak with me. They treat me like aids p...   \n",
       "4                                         I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                            body_text_nopunct  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text...   \n",
       "2                 Nah I dont think he goes to usf he lives around here though   \n",
       "3  Even my brother is not like to speak with me They treat me like aids pa...   \n",
       "4                                           I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                          body_text_tokenized  \n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, thi...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st...  \n",
       "2   [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "3  [even, my, brother, is, not, like, to, speak, with, me, they, treat, me...  \n",
       "4                                  [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['body_text_tokenized'] = dataset['body_text_nopunct'].apply(lambda x: tokenize(x.lower()))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    \n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nopunct</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather....</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, thi...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids p...</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids pa...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                    body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather....   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Tex...   \n",
       "2               Nah I don't think he goes to usf, he lives around here though   \n",
       "3  Even my brother is not like to speak with me. They treat me like aids p...   \n",
       "4                                         I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                            body_text_nopunct  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text...   \n",
       "2                 Nah I dont think he goes to usf he lives around here though   \n",
       "3  Even my brother is not like to speak with me They treat me like aids pa...   \n",
       "4                                           I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                          body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, thi...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st...   \n",
       "2   [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3  [even, my, brother, is, not, like, to, speak, with, me, they, treat, me...   \n",
       "4                                  [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                             body_text_nostop  \n",
       "0  [ive, searching, right, words, thank, breather, promise, wont, take, he...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005...  \n",
       "2                        [nah, dont, think, goes, usf, lives, around, though]  \n",
       "3                     [even, brother, like, speak, treat, like, aids, patent]  \n",
       "4                                                              [date, sunday]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['body_text_nostop'] = dataset['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "> Stemming is the process of reducing inflected or derived words to their word stem or root. More simply put, the process of stemming means often crudely chopping off the end of a word, to leave only the base. \n",
    "\n",
    "> So this means taking words with various suffixes and condensing them under the same root word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                      body_text_nostop  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['body_text_nostop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "\n",
       "                                                                                  body_text_lemmatized  \n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "1                                                    [nah, dont, think, go, usf, life, around, though]  \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]  \n",
       "3                                                                                       [date, sunday]  \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply TfidfVectorizer to smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 223)\n",
      "['08002986030', '08452810075over18s', '09061701461', '1', '100', '100000', '11', '12', '150pday', '16', '2', '20000', '2005', '21st', '3', '4', '4403LDNW1A7RW18', '4txt120', '6days', '81010', '87077', '87121', '87575', '9', '900', 'A', 'Aft', 'Alright', 'Ard', 'As', 'CASH', 'CLAIM', 'CSH11', 'Call', 'Callers', 'Callertune', 'Claim', 'Co', 'Cost', 'Cup', 'DATE', 'ENGLAND', 'Eh', 'England', 'Even', 'FA', 'FREE', 'Ffffffffff', 'Fine', 'Free', 'From', 'HAVE', 'HL', 'Had', 'He', 'I', 'Im', 'Is', 'Ive', 'Jackpot', 'KL341', 'LCCLTD', 'Macedonia', 'May', 'Melle', 'Minnaminunginte', 'Mobile', 'Nah', 'No', 'Nurungu', 'ON', 'Oh', 'Oru', 'POBOX', 'POBOXox36504W45WQ', 'Press', 'Prize', 'R', 'Reply', 'SCOTLAND', 'SIX', 'SUNDAY', 'So', 'TC', 'Text', 'That', 'The', 'Then', 'They', 'To', 'TryWALES', 'TsandCs', 'Txt', 'U', 'URGENT', 'Update', 'Valid', 'Vettam', 'WAP', 'WILL', 'WINNER', 'WITH', 'XXXMobileMovieClub', 'Yes', 'You', 'aids', 'already', 'anymore', 'apply', 'around', 'b', 'brother', 'call', 'callertune', 'camera', 'chances', 'claim', 'click', 'code', 'colour', 'comin', 'comp', 'copy', 'credit', 'cried', 'customer', 'da', 'dont', 'eg', 'enough', 'entitled', 'entry', 'feel', 'final', 'finish', 'first', 'friends', 'go', 'goalsteam', 'goes', 'going', 'gonna', 'gota', 'ha', 'home', 'hours', 'httpwap', 'info', 'joking', 'k', 'kim', 'lar', 'latest', 'like', 'link', 'lives', 'lor', 'lunch', 'make', 'meet', 'membership', 'message', 'miss', 'mobile', 'mobiles', 'months', 'name', 'national', 'naughty', 'network', 'news', 'next', 'patent', 'pay', 'per', 'pounds', 'prize', 'questionstd', 'rateTCs', 'receive', 'receivea', 'remember', 'request', 'reward', 'selected', 'send', 'seriously', 'set', 'smth', 'soon', 'sooner', 'speak', 'spell', 'stock', 'str', 'stuff', 'talk', 'team', 'think', 'though', 'tkts', 'today', 'tonight', 'treat', 'try', 'txt', 'u', 'ur', 'use', 'usf', 'v', 'valued', 'want', 'watching', 'way', 'week', 'wet', 'win', 'wkly', 'word', 'wwwdbuknet', 'xxxmobilemovieclubcomnQJKGIGHJJGCBL', '']\n"
     ]
    }
   ],
   "source": [
    "data_sample = data[0:20]\n",
    "\n",
    "tfidf_vect_sample = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf_sample = tfidf_vect_sample.fit_transform(data_sample['body_text'])\n",
    "print(X_tfidf_sample.shape)\n",
    "print(tfidf_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizers output sparse matrices\n",
    "\n",
    ">**Sparse Matrix**: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08002986030</th>\n",
       "      <th>08452810075over18s</th>\n",
       "      <th>09061701461</th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>100000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>150pday</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>watching</th>\n",
       "      <th>way</th>\n",
       "      <th>week</th>\n",
       "      <th>wet</th>\n",
       "      <th>win</th>\n",
       "      <th>wkly</th>\n",
       "      <th>word</th>\n",
       "      <th>wwwdbuknet</th>\n",
       "      <th>xxxmobilemovieclubcomnQJKGIGHJJGCBL</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.173811</td>\n",
       "      <td>0.197734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   08002986030  08452810075over18s  09061701461    1  100  100000   11   12  \\\n",
       "0          0.0            0.197734          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "1          0.0            0.000000          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "2          0.0            0.000000          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "3          0.0            0.000000          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "4          0.0            0.000000          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "\n",
       "   150pday   16  ...  watching  way  week  wet       win      wkly  word  \\\n",
       "0      0.0  0.0  ...       0.0  0.0   0.0  0.0  0.173811  0.197734   0.0   \n",
       "1      0.0  0.0  ...       0.0  0.0   0.0  0.0  0.000000  0.000000   0.0   \n",
       "2      0.0  0.0  ...       0.0  0.0   0.0  0.0  0.000000  0.000000   0.0   \n",
       "3      0.0  0.0  ...       0.0  0.0   0.0  0.0  0.000000  0.000000   0.0   \n",
       "4      0.0  0.0  ...       0.0  0.0   0.0  0.0  0.000000  0.000000   0.0   \n",
       "\n",
       "   wwwdbuknet  xxxmobilemovieclubcomnQJKGIGHJJGCBL      \n",
       "0         0.0                                  0.0  0.0  \n",
       "1         0.0                                  0.0  0.0  \n",
       "2         0.0                                  0.0  0.0  \n",
       "3         0.0                                  0.0  0.0  \n",
       "4         0.0                                  0.0  0.0  \n",
       "\n",
       "[5 rows x 223 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect_sample.get_feature_names()\n",
    "X_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the two new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_lemmatized</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent]</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...</td>\n",
       "      <td>[per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...</td>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                      body_text_nostop  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "2                                              [even, brother, like, speak, treat, like, aids, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, callers, pr...   \n",
       "\n",
       "                                                                                  body_text_lemmatized  \\\n",
       "0  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "1                                                    [nah, dont, think, go, usf, life, around, though]   \n",
       "2                                               [even, brother, like, speak, treat, like, aid, patent]   \n",
       "3                                                                                       [date, sunday]   \n",
       "4  [per, request, melle, melle, oru, minnaminunginte, nurungu, vettam, set, callertune, caller, pre...   \n",
       "\n",
       "   body_len  punct%  \n",
       "0       128     4.7  \n",
       "1        49     4.1  \n",
       "2        62     3.2  \n",
       "3        28     7.1  \n",
       "4       135     4.4  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the two new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "pyplot.hist(data['body_len'], bins)\n",
    "pyplot.title(\"Body Length Distribution\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "pyplot.hist(data['punct%'], bins)\n",
    "pyplot.title(\"Punctuation % Distribution\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the punctuation % feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-Cox Power Transformation\n",
    "\n",
    "**Base Form**: $$ y^x $$\n",
    "\n",
    "| X    | Base Form           |           Transformation               |\n",
    "|------|--------------------------|--------------------------|\n",
    "| -2   | $$ y ^ {-2} $$           | $$ \\frac{1}{y^2} $$      |\n",
    "| -1   | $$ y ^ {-1} $$           | $$ \\frac{1}{y} $$        |\n",
    "| -0.5 | $$ y ^ {\\frac{-1}{2}} $$ | $$ \\frac{1}{\\sqrt{y}} $$ |\n",
    "| 0    | $$ y^{0} $$              | $$ log(y) $$             |\n",
    "| 0.5  | $$ y ^ {\\frac{1}{2}}  $$ | $$ \\sqrt{y} $$           |\n",
    "| 1    | $$ y^{1} $$              | $$ y $$                  |\n",
    "| 2    | $$ y^{2} $$              | $$ y^2 $$                |\n",
    "\n",
    "\n",
    "**Process**\n",
    "1. Determine what range of exponents to test\n",
    "2. Apply each transformation to each value of your chosen feature\n",
    "3. Use some criteria to determine which of the transformations yield the best distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], \n",
    "                                                    data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>10174</th>\n",
       "      <th>10175</th>\n",
       "      <th>10176</th>\n",
       "      <th>10177</th>\n",
       "      <th>10178</th>\n",
       "      <th>10179</th>\n",
       "      <th>10180</th>\n",
       "      <th>10181</th>\n",
       "      <th>10182</th>\n",
       "      <th>10183</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>5.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  10186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  10174  \\\n",
       "0        19     5.3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1       121     5.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "2        32     3.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "3        23     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4        41     2.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "   10175  10176  10177     10178  10179  10180  10181  10182  10183  \n",
       "0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0  0.250709    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 10186 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 2.613 / Predict time: 0.327 ---- Precision: 1.0 / Recall: 0.834 / Accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), \n",
    "    round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 170.456 / Predict time: 0.217 ---- Precision: 0.953 / Recall: 0.834 / Accuracy: 0.973\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), \n",
    "    round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
